{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Predicting the Sale Price of Bulldozers using Machine Learning\n",
    "\n",
    "In this notebook, we're going to go through an example machine learning project with the goal of predicting the sale price of bulldozers.\n",
    "\n",
    "### 1. Problem Definition\n",
    "How well can we predict the future sale price of a bulldozer, given its characteristics and previous examples of how much similar bulldozers have been sold for?\n",
    "\n",
    "### 2. Data\n",
    "The data is downloaded from the Kaggle Bluebook for Bulldozers competition: https://www.kaggle.com/c/bluebook-for-bulldozers/data\n",
    "\n",
    "There are 3 main datasets:\n",
    "* Train.csv is the training set, which contains data through the end of 2011.\n",
    "* Valid.csv is the validation set, which contains data from January 1, 2012 - April 30, 2012\n",
    "* Test.csv is the test set, It contains data from May 1, 2012 - November 2012.\n",
    "\n",
    "### 3. Evaluation\n",
    "The evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices.\n",
    "\n",
    "For more on the evaluation of this project check: https://www.kaggle.com/c/bluebook-for-bulldozers/overview/evaluation\n",
    "\n",
    "Our goal for this project is to build a machine learning model which minimizes RMSLE.\n",
    "\n",
    "### 4. Features\n",
    "Kaggle provides a data dictionary detailing all of the features of the dataset. You can view this data dictionary on Google Sheets: https://docs.google.com/spreadsheets/d/1VfW4ukWwa29rNqX8TEgjMR1SMb1Et5_gH-W50OoYBaQ/edit?usp=sharing"
   ],
   "id": "bb3fa2e5e89996bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ],
   "id": "52139adfaf9d1bd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(\"data/TrainAndValid.csv\", low_memory=False)\n",
    "df.head()"
   ],
   "id": "5afd793ce713ab60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.info()",
   "id": "23d8660102d16999"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.isna().sum()",
   "id": "4d0276766f07a485"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.columns",
   "id": "e8e28e55d3544d43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df[\"saledate\"][:1000], df[\"SalePrice\"][:1000])\n",
    "plt.show()"
   ],
   "id": "55126ea1a5c6415d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.saledate",
   "id": "9ff569863d361236"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.SalePrice.plot.hist()\n",
    "plt.show()"
   ],
   "id": "c67b5863c747ba0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Parsing dates\n",
    "When we work with time series data, we want to enrich the time & date component as much as possible.\n",
    "\n",
    "We can do that by telling pandas which of our columns has dates in it using the `parse_dates` parameter.\n"
   ],
   "id": "414088d469e539d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import data again but this time parse dates\n",
    "df = pd.read_csv(\"data/TrainAndValid.csv\", low_memory=False, parse_dates=[\"saledate\"])"
   ],
   "id": "57a1a4d18a742533"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.saledate.dtype,",
   "id": "c836b5c69b8ed792"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.saledate[:1000]",
   "id": "a3e146ca7ffc336d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df[\"saledate\"][:1000], df[\"SalePrice\"][:1000])\n",
    "plt.show()"
   ],
   "id": "a88c4f30bae01876"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.head()",
   "id": "a9ca772b4465730b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.head().T",
   "id": "61a0072323aa96c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.saledate.head(20)",
   "id": "dc53cda6f757f13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Sort DataFrame by saledate\n",
    "When working with time series data, it's a good idea to sort it by date."
   ],
   "id": "9c4fa72fbf1934b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1e90e2eaedc55eb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sort DataFrame in date order\n",
    "df.sort_values(by=[\"saledate\"], inplace=True, ascending=True)\n",
    "df.saledate.head(20)"
   ],
   "id": "40debea201a397db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Make a copy of the original DataFrame\n",
    "\n",
    "We make a copy of the original dataframe so when we manipulate the copy, we've still got our original data.\n"
   ],
   "id": "9407ffb3c2afbf20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Make a copy\n",
    "df_tmp = df.copy()"
   ],
   "id": "288fb2a63012b08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_tmp.saledate.head(20)",
   "id": "66029f37d578fc0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Add datetime parameters for `saledate` column",
   "id": "4302c3f9029755d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_tmp[\"saleYear\"] = df_tmp.saledate.dt.year\n",
    "df_tmp[\"saleMonth\"] = df_tmp.saledate.dt.month\n",
    "df_tmp[\"saleDay\"] = df_tmp.saledate.dt.day\n",
    "df_tmp[\"saleDayOfWeek\"] = df_tmp.saledate.dt.dayofweek\n",
    "df_tmp[\"saleDayOfYear\"] = df_tmp.saledate.dt.dayofyear\n"
   ],
   "id": "2ff8a69397cf298f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_tmp.head().T",
   "id": "422ceeeff6f0953e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_tmp.drop(\"saledate\", axis=1, inplace=True)",
   "id": "cd6052161e6c04c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_tmp['state'].value_counts()",
   "id": "6f833218b3b314d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Modelling\n",
    "\n",
    "We've done enough EDA (we could always do more) but let's start to do some model-driven EDA."
   ],
   "id": "55557a15519ac8cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let's build a machine learning model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "model.fit(df_tmp.drop(\"SalePrice\", axis=1), df_tmp[\"SalePrice\"])\n",
    "\n"
   ],
   "id": "2bf283453f48539b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Convert string to categories\n",
    "\n",
    "One way we can turn all of our data into numbers is by converting them into pandas categories."
   ],
   "id": "f432ffbfd00db90c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_tmp.head().T",
   "id": "e6054e6901ab0f78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_tmp.info()",
   "id": "3d3c118a2a9aa38d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find the columns which contain strings\n",
    "for label, content in df_tmp.items():\n",
    "    if pd.api.types.is_object_dtype(content):\n",
    "        print(label)"
   ],
   "id": "b16b2e468d81b93e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This will turn all of the string value into category values\n",
    "for label, content in df_tmp.items():\n",
    "    if pd.api.types.is_object_dtype(content):\n",
    "        df_tmp[label] = content.astype(\"category\").cat.as_ordered()"
   ],
   "id": "2827edd694e5c467"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_tmp.state.cat.codes",
   "id": "ddd5bfc17f8f3dae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check missing data\n",
    "df_tmp.isnull().sum()/len(df_tmp)"
   ],
   "id": "45d6d4f3b28e12e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save preprocessed data\n",
    "df_tmp.to_csv(\"data/train_tmp.csv\", index=False)"
   ],
   "id": "999f63a158f4bf2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import preprocessed data\n",
    "df_tmp = pd.read_csv(\"data/train_tmp.csv\", low_memory=False)"
   ],
   "id": "ace0e5101a5acfb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_tmp.head()",
   "id": "bb7bda649c99d260"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fill missing values",
   "id": "a27406f7b9885599"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fill numerical missing values first",
   "id": "fa445a00f27e0697"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for label, content in df_tmp.items():\n",
    "    if pd.api.types.is_numeric_dtype(content):\n",
    "        print(label)"
   ],
   "id": "434258a48506f38e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_tmp.ModelID",
   "id": "3ff9aff7914383fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for which numeric columns have null values\n",
    "for label, content in df_tmp.items():\n",
    "    if pd.api.types.is_numeric_dtype(content):\n",
    "        if pd.isnull(content).sum():\n",
    "            print(label)"
   ],
   "id": "ca5c78a34e37c94f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fill numeric rows with the median\n",
    "for label, content in df_tmp.items():\n",
    "    if pd.api.types.is_numeric_dtype(content):\n",
    "        if pd.isnull(content).sum():\n",
    "            # Add a binary column which tells us if the data was missing\n",
    "            df_tmp[label + \"_is_missing\"] = pd.isnull(content)\n",
    "            # Fill missing numeric values with median\n",
    "            df_tmp[label] = content.fillna(content.median())"
   ],
   "id": "797f9a3486077249"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check if there's any null numeric values\n",
    "for label, content in df_tmp.items():\n",
    "    if pd.api.types.is_numeric_dtype(content):\n",
    "        if pd.isnull(content).sum():\n",
    "            print(label)"
   ],
   "id": "1935d4cff5cd3060"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_tmp.head()",
   "id": "4de40adf9044849a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check to see how many examples were missing\n",
    "df_tmp.auctioneerID_is_missing.value_counts()\n"
   ],
   "id": "e3c2b45094d2afb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_tmp.isna().sum()",
   "id": "efea0348e71a7718"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for which numeric columns have null values\n",
    "for label, content in df_tmp.items():\n",
    "    if pd.api.types.is_numeric_dtype(content):\n",
    "        if pd.isnull(content).sum():\n",
    "            print(label)\n"
   ],
   "id": "2403a87eea48e921"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fill categorical variables\n",
    "for label, content in df_tmp.items():\n",
    "    if not pd.api.types.is_numeric_dtype(content):\n",
    "        # Add binary column to indicate whether sample had missing value\n",
    "        df_tmp[label + \"_is_missing\"] = pd.isnull(content)\n",
    "        # Turn categories into numbers and add +1\n",
    "        df_tmp[label] = pd.Categorical(content).codes + 1"
   ],
   "id": "b23938302fce3f7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_tmp.info()",
   "id": "7c72989744b59eb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_tmp.head().T",
   "id": "a56d8c47c73c46f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_tmp.isna().sum()",
   "id": "2de570b4b4d576d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Now our data has no missing values\n",
    "# Let's build a machine learning model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "model.fit(df_tmp.drop(\"SalePrice\", axis=1), df_tmp[\"SalePrice\"])"
   ],
   "id": "4f68f26a96b889da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Score the model\n",
    "model.score(df_tmp.drop(\"SalePrice\", axis=1), df_tmp[\"SalePrice\"])"
   ],
   "id": "ef918165eef77aeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Splitting data into train/validation sets\n",
    "df_val = df_tmp[df_tmp.saleYear == 2012]\n",
    "df_train = df_tmp[df_tmp.saleYear != 2012]\n"
   ],
   "id": "b78ad0233340f61a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split data into X & y\n",
    "X_train, y_train = df_train.drop(\"SalePrice\", axis=1), df_train[\"SalePrice\"]\n",
    "X_valid, y_valid = df_val.drop(\"SalePrice\", axis=1), df_val[\"SalePrice\"]\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape\n"
   ],
   "id": "4b393ada1c54bd0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create an evaluation function (the competition uses RMSLE)\n",
    "from sklearn.metrics import mean_squared_log_error, mean_absolute_error\n",
    "\n",
    "def rmsle(y_test, y_preds):\n",
    "    \"\"\"\n",
    "    Calculates root mean squared log error between predictions and true labels.\n",
    "    \"\"\"\n",
    "    return np.sqrt(mean_squared_log_error(y_test, y_preds))"
   ],
   "id": "8fa1b0e7deaa0fad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a function to evaluate our\n",
    "def show_scores(model):\n",
    "    train_preds = model.predict(X_train)\n",
    "    val_preds = model.predict(X_valid)\n",
    "    scores = {\"Training MAE\": mean_absolute_error(y_train, train_preds),\n",
    "              \"Valid MAE\": mean_absolute_error(y_valid, val_preds),\n",
    "              \"Training RMSLE\": rmsle(y_train, train_preds),\n",
    "              \"Valid RMSLE\": rmsle(y_valid, val_preds)}\n",
    "    return scores\n"
   ],
   "id": "3a5005320d7f7996"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Change max_samples value\n",
    "model = RandomForestRegressor(n_jobs=-1,\n",
    "                              random_state=42,\n",
    "                              max_samples=10000)"
   ],
   "id": "1b5579d6e7f3b69e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.fit(X_train, y_train)",
   "id": "753c2b6ca420ce5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "show_scores(model)",
   "id": "5af432e449d5c901"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Hyperparameter tuning with RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Different RandomForestRegressor hyperparameters\n",
    "rf_grid = {\"n_estimators\": np.arange(10, 100, 10),\n",
    "           \"max_depth\": [None, 3, 5, 10],\n",
    "           \"min_samples_split\": np.arange(2, 20, 2),\n",
    "           \"min_samples_leaf\": np.arange(1, 20, 2),\n",
    "           \"max_features\": [0.5, 1, \"sqrt\", \"auto\"],\n",
    "           \"max_samples\": [10000]}\n",
    "\n",
    "# Instantiate RandomizedSearchCV model\n",
    "rs_model = RandomizedSearchCV(RandomForestRegressor(n_jobs=-1,\n",
    "                                                    random_state=42),\n",
    "                              param_distributions=rf_grid,\n",
    "                              n_iter=2,\n",
    "                              cv=5,\n",
    "                              verbose=True)\n",
    "\n",
    "# Fit the RandomizedSearchCV model\n",
    "rs_model.fit(X_train, y_train)\n"
   ],
   "id": "de24d8be060529e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find the best model hyperparameters\n",
    "rs_model.best_params_"
   ],
   "id": "6d83e1a3db38fe19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate the RandomizedSearch model\n",
    "show_scores(rs_model)"
   ],
   "id": "2c3ba6c397052f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train a model with the best hyperparameters\n",
    "ideal_model = RandomForestRegressor(n_estimators=20,\n",
    "                                    min_samples_leaf=8,\n",
    "                                    min_samples_split=7,\n",
    "                                    max_features=0.5,\n",
    "                                    n_jobs=-1,\n",
    "                                    max_samples=None,\n",
    "                                    random_state=42)\n",
    "\n",
    "# Fit the ideal model\n",
    "ideal_model.fit(X_train, y_train)\n"
   ],
   "id": "9b72981c44a45d8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Scores for ideal model (trained on all the data)\n",
    "\n",
    "show_scores(ideal_model)"
   ],
   "id": "561bd2613b6f7127"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import the test data\n",
    "df_test = pd.read_csv(\"data/Test.csv\", low_memory=False, parse_dates=[\"saledate\"])"
   ],
   "id": "d322275aca02165d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_test.head()",
   "id": "658209978899ba3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_test.isna().sum()",
   "id": "8cc5ddd527acf09c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Preprocessing the data (getting the test dataset in the same format as our training dataset)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Performs transformations on df and returns transformed df.\n",
    "    \"\"\"\n",
    "    df[\"saleYear\"] = df.saledate.dt.year\n",
    "    df[\"saleMonth\"] = df.saledate.dt.month\n",
    "    df[\"saleDay\"] = df.saledate.dt.day\n",
    "    df[\"saleDayOfWeek\"] = df.saledate.dt.dayofweek\n",
    "    df[\"saleDayOfYear\"] = df.saledate.dt.dayofyear\n",
    "\n",
    "    df.drop(\"saledate\", axis=1, inplace=True)\n",
    "\n",
    "    # Fill the numeric rows with median\n",
    "    for label, content in df.items():\n",
    "        if pd.api.types.is_numeric_dtype(content):\n",
    "            if pd.isnull(content).sum():\n",
    "                df[label + \"_is_missing\"] = pd.isnull(content)\n",
    "                df[label] = content.fillna(content.median())\n",
    "\n",
    "        # Filled categorical missing data and turn categories into numbers\n",
    "        if not pd.api.types.is_numeric_dtype(content):\n",
    "            df[label + \"_is_missing\"] = pd.isnull(content)\n",
    "            df[label] = pd.Categorical(content).codes + 1\n",
    "\n",
    "    return df\n",
    "\n"
   ],
   "id": "dcc34f8c4fc0cfa7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_test = preprocess_data(df_test)",
   "id": "f0f127b5a83ddf17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_test.head().T",
   "id": "c3950e0c1cab647f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We can find how the columns differ using sets\n",
    "set(X_train.columns) - set(df_test.columns)"
   ],
   "id": "17e421684c24355"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Manually adjust df_test to have auctioneerID_is_missing column\n",
    "df_test[\"auctioneerID_is_missing\"] = False\n",
    "\n",
    "df_test.head().T"
   ],
   "id": "39005c784dbb0fc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# o",
   "id": "c7b58d71afc9ab1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_test = df_test[X_train.columns]",
   "id": "64041cf857022ecc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Make predictions on the test dataset\n",
    "test_preds = ideal_model.predict(df_test)"
   ],
   "id": "c09cc3d8eaf4b72b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_test.to_csv(\"data/test_predictions.csv\", index=False)",
   "id": "3d607870dd5c35ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Feature Importance\n",
    "\n",
    "Feature importance seeks to figure out which different attributes of the data were most important when it comes to predicting the target variable (SalePrice)."
   ],
   "id": "592da7d7861251fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find feature importance of our best model\n",
    "ideal_model.feature_importances_"
   ],
   "id": "e9fba70f997e183c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Helper function for plotting feature importance\n",
    "\n",
    "def plot_features(columns, importances, n=20):\n",
    "    df = (pd.DataFrame({\"features\": columns,\n",
    "                        \"feature_importances\": importances})\n",
    "          .sort_values(\"feature_importances\", ascending=False)\n",
    "          .reset_index(drop=True))\n",
    "    # Plot the dataframe\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(df[\"features\"][:n], df[\"feature_importances\"][:20])\n",
    "    ax.set_ylabel(\"Features\")\n",
    "    ax.set_xlabel(\"Feature importance\")\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "plot_features(X_train.columns, ideal_model.feature_importances_)"
   ],
   "id": "a2511669413fc01b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bb632b79978340e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "16316027aaf832e0"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
